import sys, os
sys.path.append(os.path.dirname(__file__))

import tushare as ts
import pandas as pd
import numpy as np
from tqdm import tqdm
from utils import *
import json
import os
import matplotlib.pyplot as plt
import matplotlib.pyplot as plt
from matplotlib.cm import get_cmap
# from matplotlib.colormaps import get_cmap
from fire import Fire

# =============================================
# plt.rcParams['font.sans-serif'] = ['SimHei']
# plt.rcParams['axes.unicode_minus'] = False 
# plt.rcParams['font.sans-serif'] = ['Arial Unicode MS']

# plt.rcParams['font.sans-serif'] = ['SimHei']
# plt.rcParams['axes.unicode_minus'] = False

# # 0.è®¾ç½®ä¸­æ–‡å­—ä½“
# import platform
# system = platform.system()
# if system == 'Darwin':  # macOS
#     plt.rcParams['font.sans-serif'] = ['Arial Unicode MS']
# elif system == 'Linux':  # Linux/Colab
#     plt.rcParams['font.sans-serif'] = ['Noto Sans CJK SC']
# elif system == 'Windows':
#     plt.rcParams['font.sans-serif'] = ['SimHei']
# else:
#     plt.rcParams['font.sans-serif'] = ['DejaVu Sans']
# plt.rcParams['axes.unicode_minus'] = False

# =============================================
# path = os.getcwd()
# folder_path = path+'/db_file/'
# # folder_path = os.environ.get("DB_PATH", path + '/db_file/')
# file_path = f'sqlite:////{folder_path}'

# script_dir  = os.path.dirname(os.path.abspath(__file__))
# folder_path = os.path.join(os.path.dirname(script_dir), 'db_file/')
# file_path    = f'sqlite:////{folder_path}'
# =============================================

import matplotlib
import platform

system = platform.system()
if system == "Darwin":  # macOS
    font = "Arial Unicode MS"
elif system == "Linux":  # Linux/Colab
    font = "Noto Sans CJK SC"
elif system == "Windows":
    font = "SimHei"
else:
    font = "DejaVu Sans"

# âš ï¸ å¦‚æœè¿™ä¸ªå­—ä½“ä¸å­˜åœ¨ï¼Œå°± fallback åˆ° matplotlib é»˜è®¤å­—ä½“
if font not in matplotlib.font_manager.findSystemFonts(fontpaths=None, fontext="ttf"):
    font = "DejaVu Sans"

plt.rcParams["font.sans-serif"] = [font]
plt.rcParams["axes.unicode_minus"] = False




# 1. å…ˆçœ‹ç¯å¢ƒå˜é‡ï¼›æ²¡æœ‰å†é€€å›åˆ°é»˜è®¤ç›¸å¯¹è·¯å¾„
folder_path = os.getenv("DB_FILE") or os.path.join(os.path.dirname(os.path.abspath(__file__)), '..', 'db_file')

# 2. ä¿è¯ç»å¯¹è·¯å¾„ & ä»¥ '/' ç»“å°¾
folder_path = os.path.abspath(folder_path.rstrip('/')) + '/'

# 3. utils.get_data_by_sql ä¼šè‡ªè¡Œæ‹¼æ¥ xxx.dbï¼Œæ‰€ä»¥è¿™é‡Œåªè¦å‰ç¼€
file_path   = f"sqlite:////{folder_path}"



def get_æŒ‡æ ‡(rr, port):
    from itertools import accumulate
    def BARSLAST(series_1):
        return pd.Series(np.array(list(accumulate(~series_1, lambda x, y: (x + y) * y))), index=series_1.index)

    å‡€å€¼æ›²çº¿ = 1 + rr.cumsum()
    æ—¶é—´è·¨åº¦ = len(rr) / 12
    æ€»æ”¶ç›Š = rr.sum()
    å¹´åŒ–æ”¶ç›Š = æ€»æ”¶ç›Š / æ—¶é—´è·¨åº¦
    å¹´åŒ–æ³¢åŠ¨ = rr.std() * (12 ** 0.5)
    å¤æ™®æ¯”ç‡ = å¹´åŒ–æ”¶ç›Š / å¹´åŒ–æ³¢åŠ¨
    æœ€å¤§å›æ’¤ = abs((rr.cumsum() - rr.cumsum().cummax()).min())
    å¡ç›æ¯”ç‡ = å¹´åŒ–æ”¶ç›Š / æœ€å¤§å›æ’¤
    æœ€å¤§ä¸‹æ½œæœŸ = BARSLAST(å‡€å€¼æ›²çº¿ / å‡€å€¼æ›²çº¿.cummax() == 1).max()
    dd_index = pro.index_daily(ts_code='399300.SZ')  # æ²ªæ·±300
    dd_index.index = pd.to_datetime(dd_index['trade_date'])
    dd_index = dd_index.sort_index()
    dd_index = dd_index['close']
    dd_index = dd_index.resample('ME').last().pct_change()
    dd_index = dd_index[port.index[0]:port.index[-1]]
    åŸºå‡†æ€»æ”¶ç›Š = dd_index.sum()
    åŸºå‡†å¹´åŒ–æ”¶ç›Š = åŸºå‡†æ€»æ”¶ç›Š / æ—¶é—´è·¨åº¦
    å¹´åŒ–è¶…é¢æ”¶ç›Š = å¹´åŒ–æ”¶ç›Š - åŸºå‡†å¹´åŒ–æ”¶ç›Š

    xx1 = ['æ—¶é—´è·¨åº¦', 'å¹´åŒ–æ”¶ç›Š', 'å¹´åŒ–è¶…é¢æ”¶ç›Š', 'å¹´åŒ–æ³¢åŠ¨', 'å¤æ™®æ¯”ç‡', 'æœ€å¤§å›æ’¤', 'å¡ç›æ¯”ç‡', 'æœ€å¤§ä¸‹æ½œæœŸ']
    xx2 = [æ—¶é—´è·¨åº¦, å¹´åŒ–æ”¶ç›Š, å¹´åŒ–è¶…é¢æ”¶ç›Š, å¹´åŒ–æ³¢åŠ¨, å¤æ™®æ¯”ç‡, æœ€å¤§å›æ’¤, å¡ç›æ¯”ç‡, æœ€å¤§ä¸‹æ½œæœŸ, åŸºå‡†å¹´åŒ–æ”¶ç›Š]
    return dict(zip(xx1, xx2))


def è·å–æŒ‡æ•°(df_ports):
    df_ä¸Šè¯æŒ‡æ•° = pro.index_daily(ts_code='000001.SH')
    df_æ²ªæ·±300 = pro.index_daily(ts_code='399300.SZ')
    df_ä¸Šè¯50 = pro.index_daily(ts_code='000016.SH')
    df_åˆ›ä¸š = pro.index_daily(ts_code='399006.SZ')

    df_ä¸Šè¯æŒ‡æ•°.index = pd.to_datetime(df_ä¸Šè¯æŒ‡æ•°['trade_date'])
    df_æ²ªæ·±300.index = pd.to_datetime(df_æ²ªæ·±300['trade_date'])
    df_ä¸Šè¯50.index = pd.to_datetime(df_ä¸Šè¯50['trade_date'])
    df_åˆ›ä¸š.index = pd.to_datetime(df_åˆ›ä¸š['trade_date'])

    df_ä¸Šè¯æŒ‡æ•° = df_ä¸Šè¯æŒ‡æ•°.sort_index()
    df_æ²ªæ·±300 = df_æ²ªæ·±300.sort_index()
    df_ä¸Šè¯50 = df_ä¸Šè¯50.sort_index()
    df_åˆ›ä¸š = df_åˆ›ä¸š.sort_index()

    df_ä¸Šè¯æŒ‡æ•° = df_ä¸Šè¯æŒ‡æ•°['close']
    df_æ²ªæ·±300 = df_æ²ªæ·±300['close']
    df_ä¸Šè¯50 = df_ä¸Šè¯50['close']
    df_åˆ›ä¸š = df_åˆ›ä¸š['close']

    df_æŒ‡æ•° = pd.concat([df_ä¸Šè¯æŒ‡æ•°, df_æ²ªæ·±300, df_ä¸Šè¯50, df_åˆ›ä¸š], axis=1)
    df_æŒ‡æ•°.columns = ['SCI', 'CSI300', 'SSE50', 'CNX']
    df_æŒ‡æ•° = df_æŒ‡æ•°.resample('ME').last().pct_change()
    df_æŒ‡æ•° = df_æŒ‡æ•°[df_ports.index[0]:df_ports.index[-1]]

    return df_æŒ‡æ•°

def calculate_accuracy(df):
    accuracy_dict = {}

    for column in df.columns:
        if column in df.columns:
            accuracy = (df[column] == df['ground_truth']).mean()
            accuracy_dict[column] = accuracy
    return accuracy_dict

def main(tushare_token, stockgpt_mldl_path, save_dir, file_name):
    global pro
    token = tushare_token
    pro = ts.pro_api(token)

    å¤šç©ºç±»å‹ = 'å¤šç©ºéƒ½å¯'  # ä»…é™åšå¤š,ä»…é™åšç©º å¤šç©ºéƒ½å¯
    weight = 'å¸‚å€¼åŠ æƒ'  #å¸‚å€¼åŠ æƒ,å¹³å‡åŠ æƒ

    dd1 = pd.read_excel(stockgpt_mldl_path, engine='openpyxl')

    dd1['date'] = pd.to_datetime(dd1['date'])
    dd = dd1
    dd['stock_name'] = dd['stock_name'].replace('äº‘æµ·é‡‘å±', 'å®æ­¦é•ä¸š')

    dd = dd.rename(columns={'Transformers':'Bert','chatglm2_6b_greedy':'chatglm2','fingpt_greedy':'FinGPT','finma_greedy':'FinMA'})

    print(f"dd1: {dd1}")

    accuracy = {}

    if å¤šç©ºç±»å‹ == 'å¤šç©ºéƒ½å¯':
        for column in dd.columns[4:]:
            filtered_df = dd[dd[column] != 0]
            correct_predictions = (filtered_df['ground_truth'] == filtered_df[column]).sum() 
            total_predictions = len(filtered_df)  
            accuracy[column] = f'{round((float(correct_predictions) / total_predictions)*100,2)}%'
        
    elif å¤šç©ºç±»å‹ == 'ä»…é™åšå¤š':
        for column in dd.columns[4:]:
            filtered_df = dd[(dd[column] == 1)]  
            correct_predictions = (filtered_df['ground_truth'] == filtered_df[column]).sum()
            print("correct_predictions",correct_predictions)
            total_predictions = len(dd[dd[column] == 1])
            print("total_predictions",total_predictions)
            accuracy[column] = f'{round((float(correct_predictions) / total_predictions)*100,2)}%'

    else :
        for column in dd.columns[4:]:
            filtered_df = dd[(dd[column] == -1)] 
            correct_predictions = (filtered_df['ground_truth'] == filtered_df[column]).sum()
            print("correct_predictions",correct_predictions)
            total_predictions = len(dd[dd[column] == -1])
            print("total_predictions",total_predictions)
            accuracy[column] = f'{round((float(correct_predictions) / total_predictions)*100,2)}%'


    accuracy_df = pd.DataFrame.from_dict(accuracy, orient='index', columns=['accuracy'])
    print(accuracy_df)


    field_names = dd.columns[4:].tolist()


    dd_stock = pro.stock_basic(exchange='')

    # temporary patch: changes to some stock info
    # TODO: remove tushare api dependencies
    change_stock_name = {
        "*STä¸œå›­": "ä¸œæ–¹å›­æ—",
        "ä¸­äº¤è®¾è®¡": "ç¥è¿å±±",
        "å¹¿ä¸œå»ºå·¥": "ç²¤æ°´ç”µ",
        "é‡‘ç‰Œå®¶å±…": "é‡‘ç‰Œå¨æŸœ",
        "*STé‡‘ç§‘": "é‡‘ç§‘è‚¡ä»½"
    }
    for k,v in change_stock_name.items():
        dd_stock["name"] = dd_stock["name"].replace([k], v)
    new_row = pd.Series(["000961.SZ", "000961", "ä¸­å—å»ºè®¾", None, None, None, None, None, None, None], index=dd_stock.columns)
    # dd_stock = dd_stock.append(new_row.to_frame().T)
    dd_stock = pd.concat([dd_stock, new_row.to_frame().T], ignore_index=True)


    name_code_dict = dict(zip(dd_stock['name'], dd_stock['ts_code']))
    code_name_dict = dict(zip(dd_stock['ts_code'], dd_stock['name']))
    codes = dd_stock[dd_stock['name'].isin(dd['stock_name'])]['ts_code'].tolist()
    dd['stock_code'] = dd['stock_name'].apply(lambda x: name_code_dict.get(x))
    dd['next_month'] = pd.to_datetime(dd['date']) + pd.offsets.DateOffset(months=1) + pd.offsets.MonthEnd(1)

    print(f"\n[NOTICE] Loading db files, please wait a moment...\n")
    df_adj = get_data_by_sql(file_path, 'daily_adj', 'daily_adj', codes, '*')
    df_kline = get_data_by_sql(file_path, 'daily_kline', 'daily_kline', codes, '*')
    df_dailybasic = get_data_by_sql(file_path, 'dailybasic', 'dailybasic', codes,
                                    'ts_code,trade_date,total_mv,pe_ttm,pb,dv_ttm')

    df_adj = get_pivot_data(df_adj, 'adj_factor')
    df_close = get_pivot_data(df_kline, 'close')
    df_close = (df_close * df_adj / df_adj.loc[df_adj.index[-1]]).round(2)
    df_close.columns = [code_name_dict.get(x) for x in df_close.columns]
    MV = get_pivot_data(df_dailybasic, 'total_mv')
    MV.columns = [code_name_dict.get(x) for x in MV.columns]

    ports = []
    for field_name in tqdm(field_names):
        ddx = dd[['stock_name', 'next_month', field_name]].drop_duplicates(subset=['stock_name', 'next_month'],
                                                                        keep='first')  # å–ç¬¬ä¸€ä¸ª
        ddx = ddx.pivot(index='next_month', columns='stock_name', values=field_name).fillna(0)
        MV = MV.resample('ME').last()
        # MV = MV[ddx.columns]
        # ğŸ” æ£€æŸ¥å¹¶è¿‡æ»¤æ‰ MV ä¸­æ²¡æœ‰çš„è‚¡ç¥¨ï¼Œé¿å… KeyError
        missing_cols = ddx.columns.difference(MV.columns)
        if not missing_cols.empty:
          print(f"[WARNING] Missing columns in MV: {list(missing_cols)}")

          print("ç¼ºå¤±è‚¡ç¥¨åç§° -> ts_code æ˜ å°„å¦‚ä¸‹ï¼š")
          for name in missing_cols:
              print(name, '->', name_code_dict.get(name))

          # âœ… å®‰å…¨è¿‡æ»¤æ‰ç¼ºå¤±åˆ—
          ddx = ddx.loc[:, ddx.columns.intersection(MV.columns)]
          MV = MV[ddx.columns]
        
        MV = MV[MV.index.isin(ddx.index)]

        # dd_ret = df_close.resample('ME').last().pct_change()
        dd_ret = df_close.resample('ME').last().pct_change(fill_method=None)
        dd_ret = dd_ret[ddx.columns]
        dd_ret = dd_ret[dd_ret.index.isin(ddx.index)]

        MV = MV.astype(float)
        ddx = ddx.astype(float)
        dd_ret = dd_ret.astype(float)

        if å¤šç©ºç±»å‹ == 'ä»…é™åšå¤š':
            ddx = ddx * (ddx > 0)
        elif å¤šç©ºç±»å‹ == 'ä»…é™åšç©º':
            ddx = ddx * (ddx < 0)

        if weight =='å¹³å‡åŠ æƒ':
            port = (dd_ret * ddx).sum(1) / ddx.abs().sum(1)  # ç®€å•å¹³å‡çš„ç»„åˆæ”¶ç›Šç‡
    
        else:
            port = (dd_ret*ddx*(MV*ddx.abs()).div((MV*ddx.abs()).sum(1),axis=0)).sum(1) 
        ports.append(port)

    df_ports = pd.concat(ports, axis=1)
    df_ports.columns = field_names


    df_æŒ‡æ•° = è·å–æŒ‡æ•°(df_ports)


    æŒ‡æ ‡s = []
    for field_name in field_names:
        æŒ‡æ ‡s.append(get_æŒ‡æ ‡(df_ports[field_name], port))
    for x in df_æŒ‡æ•°.columns:
        æŒ‡æ ‡s.append(get_æŒ‡æ ‡(df_æŒ‡æ•°[x], port))



    df_æŒ‡æ ‡ = pd.DataFrame(æŒ‡æ ‡s).T
    df_æŒ‡æ ‡.columns = field_names + list(df_æŒ‡æ•°.columns)
    df_æŒ‡æ ‡ = df_æŒ‡æ ‡.round(3)
    print("df_æŒ‡æ ‡",df_æŒ‡æ ‡)

    d0 = pd.DataFrame(dict(zip(field_names, [[0]] * len(field_names))),
                    index=[df_ports.index[0] - pd.offsets.DateOffset(months=1) + pd.offsets.MonthEnd(0)])
    df_ports = pd.concat([d0, df_ports])

    d0 = pd.DataFrame(dict(zip(df_æŒ‡æ•°.columns, [[0]] * len(df_æŒ‡æ•°.columns))),
                    index=[df_æŒ‡æ•°.index[0] - pd.offsets.DateOffset(months=1) + pd.offsets.MonthEnd(0)])
    df_æŒ‡æ•° = pd.concat([d0, df_æŒ‡æ•°])



    fig, ax = plt.subplots(figsize=(10, 4))
    cmap = get_cmap('tab10')
    line_styles = ['-', '--', '-.', ':']


    lines_ports = []
    for i, col in enumerate(df_ports.columns):
        line, = ax.plot(df_ports[col].cumsum(), color=cmap(i % cmap.N), linestyle=line_styles[i % len(line_styles)])
        lines_ports.append(line)


    lines_æŒ‡æ•° = []
    for i, col in enumerate(df_æŒ‡æ•°.columns):
        line, = ax.plot(df_æŒ‡æ•°[col].cumsum(), color=cmap((i+len(df_ports.columns)) % cmap.N),
                linestyle=line_styles[(i+len(df_ports.columns)) % len(line_styles)])
        lines_æŒ‡æ•°.append(line)
        

    lines = lines_ports + lines_æŒ‡æ•°

    labels = list(df_ports.columns) + list(df_æŒ‡æ•°.columns)
    sorted_lines_labels = sorted(zip(lines, labels), key=lambda x: x[0].get_ydata()[-1], reverse=True)
    sorted_lines, sorted_labels = zip(*sorted_lines_labels)


    for line, label in zip(lines, labels):
        line.set_linewidth(1)  
        # if label == "Stock-Chain":
        #     line.set_linestyle('-')
        #     line.set_linewidth(2)
        #     line.set_color('IndianRed')

        if label == "StockGPT_RAG":
            line.set_linestyle('-')  # å®çº¿
            line.set_linewidth(2.5)  # åŠ ç²—
            line.set_color('red')  # çº¢è‰²
        elif label == "Stock-Chain":
            line.set_linestyle('-')  
            line.set_linewidth(2)
            line.set_color('IndianRed')

    ax.legend(sorted_lines, sorted_labels, bbox_to_anchor=(1.12, 0.9), loc='upper right')

    ax.grid()
    ax.tick_params(axis='x', labelsize=10)
    ax.tick_params(axis='y', labelsize=10)

    plt.tight_layout()  # å¯é€‰ï¼Œè‡ªåŠ¨å¸ƒå±€
    plt.savefig(f"{save_dir}/{file_name}.png", dpi=500)
    # plt.show()        # å¦‚æœéœ€è¦åœ¨ Notebookæ˜¾ç¤ºï¼Œå»æ‰è„šæœ¬æ³¨æ‰
    plt.close()

    df_æŒ‡æ ‡.to_csv(f"{save_dir}/{file_name}-table.csv")
    json_result = df_æŒ‡æ ‡['StockGPT_RAG'].to_json()
    with open(os.path.join(save_dir, 'all_result.jsonl'), 'a+') as f:
        f.write(json.dumps(json_result) + '\n')

    print('-----finish-----\n\n')

if __name__ == '__main__':
    Fire(main)